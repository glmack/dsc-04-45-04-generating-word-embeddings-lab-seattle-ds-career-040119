{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Word Embeddings - Lab\n",
    "\n",
    "Generate word embeddings by training Word2Vec model and by building embedding layers into deep neural networks. In this lab, we'll start by creating our own word embeddings by making use of the Word2Vec Model. Then, we'll move onto building Neural Networks that make use of **_Embedding Layers_** to accomplish the same end-goal, but directly in our model. \n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Demonstrate a basic understanding of the architecture of the Word2Vec model\n",
    "* Demonstrate an understanding of the various tunable parameters of word2vec such as vector size and window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and unizip data stored in the compressed file 'News_Category_Dataset_v2.json'\n",
    "The easiest way to make use of Word2Vec is to import it from the [Gensim Library](https://radimrehurek.com/gensim/). This model contains a full implementation of Word2Vec, which we can use to begin training immediately. For this lab, we'll be working with the [News Category Dataset from Kaggle](https://www.kaggle.com/rmisra/news-category-dataset/version/2#_=_).  This dataset contains headlines and article descriptions from the news, as well as categories for which type of article they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip \"News_Category_Dataset_v2.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts JSON string to pandas object, lines=True reads the file as a json object per line\n",
    "raw_df = pd.read_json(\"News_Category_Dataset_v2.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89</td>\n",
       "      <td>She left her husband. He killed their children. Just another day in America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smith-joins-diplo-and-nicky-jam-for-the-official-2018-world-cup-song_us_5b09726fe4b0fdb2aa541201</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-grant-marries_us_5b09212ce4b0568a880b9a8c</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen  CRIME         2018-05-26   \n",
       "1  Andy McDonald    ENTERTAINMENT 2018-05-26   \n",
       "2  Ron Dicker       ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                                                      headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV              \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song   \n",
       "2  Hugh Grant Marries For The First Time At Age 57                               \n",
       "\n",
       "                                                                                                                                         link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89                                          \n",
       "1  https://www.huffingtonpost.com/entry/will-smith-joins-diplo-and-nicky-jam-for-the-official-2018-world-cup-song_us_5b09726fe4b0fdb2aa541201   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-grant-marries_us_5b09212ce4b0568a880b9a8c                                                          \n",
       "\n",
       "                                                                         short_description  \n",
       "0  She left her husband. He killed their children. Just another day in America.             \n",
       "1  Of course it has a song.                                                                 \n",
       "2  The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "- Create a column called `combined_text` that consists of the data from `df.headline` plus a space character (`' '`) plus the data from `df.short_description`.\n",
    "- Uses the `combined_text` column's `map()` function and pass in `word_tokenize`. Store the result returned in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89</td>\n",
       "      <td>She left her husband. He killed their children. Just another day in America.</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV She left her husband. He killed their children. Just another day in America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smith-joins-diplo-and-nicky-jam-for-the-official-2018-world-cup-song_us_5b09726fe4b0fdb2aa541201</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-grant-marries_us_5b09212ce4b0568a880b9a8c</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen  CRIME         2018-05-26   \n",
       "1  Andy McDonald    ENTERTAINMENT 2018-05-26   \n",
       "2  Ron Dicker       ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                                                      headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV              \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song   \n",
       "2  Hugh Grant Marries For The First Time At Age 57                               \n",
       "\n",
       "                                                                                                                                         link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89                                          \n",
       "1  https://www.huffingtonpost.com/entry/will-smith-joins-diplo-and-nicky-jam-for-the-official-2018-world-cup-song_us_5b09726fe4b0fdb2aa541201   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-grant-marries_us_5b09212ce4b0568a880b9a8c                                                          \n",
       "\n",
       "                                                                         short_description  \\\n",
       "0  She left her husband. He killed their children. Just another day in America.              \n",
       "1  Of course it has a song.                                                                  \n",
       "2  The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.   \n",
       "\n",
       "                                                                                                                                   combined_text  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV She left her husband. He killed their children. Just another day in America.  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song Of course it has a song.                                           \n",
       "2  Hugh Grant Marries For The First Time At Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['combined_text'] = raw_df.headline + ' ' + raw_df.short_description\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV She left her husband. He killed their children. Just another day in America.\n",
       "1    Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song Of course it has a song.                                         \n",
       "2    Hugh Grant Marries For The First Time At Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.      \n",
       "Name: combined_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.combined_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values of Series according to input correspondence.\n",
    "# substitute each value in series derived from NLTK word_tokenize function\n",
    "data = raw_df['combined_text'].map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [There, Were, 2, Mass, Shootings, In, Texas, Last, Week, ,, But, Only, 1, On, TV, She, left, her, husband, ., He, killed, their, children, ., Just, another, day, in, America, .]\n",
       "1    [Will, Smith, Joins, Diplo, And, Nicky, Jam, For, The, 2018, World, Cup, 's, Official, Song, Of, course, it, has, a, song, .]                                                    \n",
       "2    [Hugh, Grant, Marries, For, The, First, Time, At, Age, 57, The, actor, and, his, longtime, girlfriend, Anna, Eberstein, tied, the, knot, in, a, civil, ceremony, .]              \n",
       "Name: combined_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first 3 items in `data` to see how everything looks \n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenized words need to be in their original order for Word2Vec to establish the meaning of them.\n",
    "- Word2Vec model learns excellent vector representations for each word in an **_Embedding Space_**, where the relationships between vectors capture semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiates word2vec model\n",
    "# window: maximum distance between the current and predicted word within a sentence\n",
    "# size: dimensionality of the word vectors\n",
    "# min_count: min number of times (frequency) a word must appear in vocab for inclusion in model\n",
    "# workers param: number of worker threads to train the model, for faster training with multicore machines\n",
    "model_w2v = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200853"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'corpus_count' returns number of sentences in dataset, in this case, 200K sentences\n",
    "model_w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55562498, 67352790)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train updates the model’s neural weights from a sequence of sentences\n",
    "# training is streamed, meaning sentences can be a generator that reads input data from disk on-the-fly,\n",
    "# without loading the entire corpus into RAM. This also means you can continue training the model later:\n",
    "\n",
    "model_w2v.train(data, total_examples=model_w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a fully trained model! The word vectors themselves are stored inside of a `Word2VecKeyedVectors` instance, which we'll find stored inside of `model.wv`. For simplicity's sake, let's go ahead and store this inside of the variable `wv` in order to save ourselves some keystrokes down the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .wv separates trained word vectors in a KeyedVectors instance and assigns to var so don't need full model state\n",
    "# (don’t need to continue training) by discarding state, we have a much smaller and faster object that can be\n",
    "# mapped for fast loading and sharing the vectors in RAM between processes\n",
    "\n",
    "word_vectors = model_w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2VecTrainables at 0x1a33b42320>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.trainables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words_list = ['guns', 'Houston', 'border', 'Mexico', 'barbecue', 'Austin', 'desert', 'hot', 'south', 'cowboy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('City', 0.00070574536),\n",
       " ('north', 0.00052461616),\n",
       " ('town', 0.0003454863),\n",
       " ('south', 0.00033197782),\n",
       " ('northern', 0.00031866922),\n",
       " ('desert', 0.00030388113),\n",
       " ('near', 0.00029936808),\n",
       " ('eastern', 0.00027175437),\n",
       " ('coast', 0.00026903074),\n",
       " ('island', 0.00026366086)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the probability distribution of the center word given context words\n",
    "model_w2v.predict_output_word(context_words_list, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore our word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Louisiana', 0.826757550239563),\n",
       " ('Pennsylvania', 0.8239947557449341),\n",
       " ('Ohio', 0.8204474449157715),\n",
       " ('Oregon', 0.8143745064735413),\n",
       " ('Maryland', 0.8141811490058899),\n",
       " ('Massachusetts', 0.8063214421272278),\n",
       " ('California', 0.8054158687591553),\n",
       " ('Illinois', 0.799420952796936),\n",
       " ('Florida', 0.7938308715820312),\n",
       " ('Utah', 0.7913682460784912)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - wv.most_similar() computes cosine similarity & returns most similar words to a word passed to function\n",
    "\n",
    "# finds top-N most similar docvecs from training set. Positive docvecs contribute positively towards the similarity,\n",
    "# negative docvecs negatively.\n",
    "\n",
    "# This method computes cosine similarity between a simple mean of the projection weight vectors of the given docs.\n",
    "# Docs may be specified as vectors, integer indexes of trained docvecs, or if the documents were originally presented with string tags, by the corresponding tags.\n",
    "\n",
    "word_vectors.most_similar(positive='Texas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- result: most similar words are also states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can get the least similar vectors to a given word by passing in the word to the `most_similar()` function's `negative` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Parent/Grandparent', 0.45414793491363525),\n",
       " ('once-reliable', 0.4304124712944031),\n",
       " ('Sis-In-Law', 0.39956024289131165),\n",
       " ('Optimizing', 0.39162325859069824),\n",
       " ('Headstrong', 0.3843386173248291),\n",
       " ('intercultural', 0.3744005560874939),\n",
       " ('Hunger-Free', 0.36647677421569824),\n",
       " ('Depriving', 0.3639453053474426),\n",
       " ('uh-oh', 0.3557976484298706),\n",
       " ('Much-ness', 0.35288339853286743)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top-N most similar docvecs from the training set. \n",
    "# positive docvecs contribute positively towards the similarity, negative docvecs negatively.\n",
    "\n",
    "# computes cosine similarity between a simple mean of the projection weight vectors of the given docs. \n",
    "# Docs may be specified as vectors, integer indexes of trained docvecs, \n",
    "# or if the documents were originally presented with string tags, by the corresponding tags.\n",
    "\n",
    "word_vectors.most_similar(negative='Texas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the closest vectors in the embedding space will almost certainly share some level of semantic meaning with a given word, there is no guarantee that this relationship will hold at large distances. \n",
    "\n",
    "We can also get the vector for a given word by passing in the word as if we were passing in a key to a dictionary. \n",
    "\n",
    "In the cell below, get the word vector for `'Texas'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5758936 ,  0.46962196,  0.33029893, -2.5163448 , -2.2553284 ,\n",
       "       -0.68543625,  1.8183864 , -2.970106  ,  0.19633321,  1.0542854 ,\n",
       "       -1.7109283 ,  1.6196697 ,  0.12729505, -1.5691562 , -2.5476475 ,\n",
       "        0.24189892,  1.5157443 , -1.5040779 ,  0.4470289 ,  0.6064744 ,\n",
       "        2.5843513 , -1.2880641 ,  1.5115134 ,  1.1049927 ,  0.38054934,\n",
       "       -0.1607801 , -0.9859167 , -1.1762999 , -0.882581  ,  0.966137  ,\n",
       "        0.87757045,  0.8092765 , -0.15343319, -1.7439047 , -1.9013453 ,\n",
       "       -0.7291452 ,  1.9337308 ,  0.3165315 , -2.3144536 , -0.5231052 ,\n",
       "       -0.33261743,  2.1900227 , -2.1479876 ,  1.1273638 ,  2.0968862 ,\n",
       "       -0.23423743,  0.1549269 , -0.2763069 ,  1.0159162 , -1.334968  ,\n",
       "        1.5590531 ,  0.9680792 , -0.8537756 ,  0.25349402, -0.55311173,\n",
       "        0.28010517,  0.17754398,  0.27995607, -1.5487891 ,  1.9830393 ,\n",
       "       -1.2884064 ,  0.642198  ,  1.1545573 , -0.32478595, -0.50944614,\n",
       "        0.5767471 ,  1.8612963 ,  0.21740426, -1.9394921 , -0.59445876,\n",
       "       -2.1464698 ,  1.2574527 ,  2.2221863 ,  0.49084747, -0.34874442,\n",
       "       -0.7916692 ,  0.91564196, -0.22746287, -0.1518535 ,  0.2903444 ,\n",
       "       -0.31891486, -4.1441565 ,  1.4811034 , -0.90222484, -0.6224304 ,\n",
       "        0.18050168, -1.6876526 ,  1.6477096 , -0.9021248 , -0.6050794 ,\n",
       "       -1.7841469 ,  0.03991784, -0.37146205,  1.7305939 ,  0.4657468 ,\n",
       "        0.11040314, -1.5303587 ,  0.67635095, -1.1041174 , -1.0278174 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25651667,  0.5139187 ,  1.159203  , ..., -0.0859317 ,\n",
       "         0.90008134, -1.1546249 ],\n",
       "       [-0.1313287 ,  1.0463067 ,  1.2342092 , ...,  0.34663165,\n",
       "         1.0347157 , -0.8554366 ],\n",
       "       [ 2.026801  ,  3.8459706 ,  0.6995039 , ..., -1.1599364 ,\n",
       "        -0.14289753,  0.15876597],\n",
       "       ...,\n",
       "       [ 0.03125991, -0.14082758, -0.08043785, ..., -0.0153221 ,\n",
       "         0.01296047, -0.01258767],\n",
       "       [ 0.07368399, -0.06965393,  0.04993903, ..., -0.00656883,\n",
       "         0.02755367,  0.07161842],\n",
       "       [-0.04707934, -0.1561053 , -0.02667405, ..., -0.01223683,\n",
       "         0.0153603 , -0.02103641]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns word vectors for entire vocabulary(dictionary)\n",
    "word_vectors.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, let's try recreating the _'king' - 'man' + 'woman' = 'queen'_ example we've seen before. We can do this by using the `most_similar` function and putting the things we want added together inside of an array passed to the `positive` parameter, and the things we want subtracted as an array passed to the the `negative` parameter. \n",
    "\n",
    "Do this now in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, our model isn't perfect, but 'Queen' is still in the top 3, and with 'Princess' not too far behind. As we can see from the word in first place, 'reminiscent', our model is far from perfect. This is likely because we didn't give it too much training, or training data. However, for the small amount of training data it was given, the model still performs remarkably well! \n",
    "\n",
    "We'll see in the next lab that from a practical standpoint, one of the best things we can do for performance is to start by loading in the weights from an open-sourced model that has been trained for a very long time on a massive amount of data, such as the GloVe model from the Stanford NLP Group. There's not really any benefit from training the model ourselves, unless our text uses different, specialized vocabulary that isn't likely to be well represented inside an open-source model.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we learned how to train and use a Word2Vec model to created vectorized word embeddings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.5"
=======
   "version": "3.6.6"
>>>>>>> c8b20246e515b4c394fdeb42754a5c836a8b8bbe
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
